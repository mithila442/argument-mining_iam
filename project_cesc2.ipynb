{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Restrict to GPU 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:18:59.335762Z",
     "start_time": "2024-12-01T22:18:59.015062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of stance_label (first instance): <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"train.txt\"  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path, delimiter=\"\\t\", header=None, names=[\n",
    "    'claim_label', 'topic_sentence', 'claim_candidate_sentence', 'article_id', 'stance_label'\n",
    "])\n",
    "\n",
    "# Check the data type of the first value in the 'stance_label' column\n",
    "print(\"Data type of stance_label (first instance):\", type(data['article_id'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:19:13.975913Z",
     "start_time": "2024-12-01T22:19:05.792587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed Dataset:\n",
      "   claim_label                          topic_sentence  \\\n",
      "0            0  Should we abandon the one-child policy   \n",
      "1            0  Should we abandon the one-child policy   \n",
      "2            0  Should we abandon the one-child policy   \n",
      "3            0  Should we abandon the one-child policy   \n",
      "4            0  Should we abandon the one-child policy   \n",
      "\n",
      "                            claim_candidate_sentence article_id  stance_label  \\\n",
      "0                                      Sex selection        1_1             1   \n",
      "1  Sex selection is the attempt to control the se...        1_1             1   \n",
      "2  It can be accomplished in several ways, both p...        1_1             1   \n",
      "3  It has been marketed under the title family ba...        1_1             1   \n",
      "4  According to the United Nations Population Fun...        1_1             1   \n",
      "\n",
      "   joint_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "\n",
      "Dimensions of Unprocessed Dataset:\n",
      "Rows: 69666, Columns: 6\n",
      "\n",
      "Cleaned Dataset:\n",
      "   claim_label                          topic_sentence  \\\n",
      "0            0  Should we abandon the one-child policy   \n",
      "1            0  Should we abandon the one-child policy   \n",
      "2            0  Should we abandon the one-child policy   \n",
      "3            0  Should we abandon the one-child policy   \n",
      "4            0  Should we abandon the one-child policy   \n",
      "\n",
      "                            claim_candidate_sentence article_id  stance_label  \\\n",
      "0                                      Sex selection        1_1             1   \n",
      "1  Sex selection is the attempt to control the se...        1_1             1   \n",
      "2  It can be accomplished in several ways, both p...        1_1             1   \n",
      "3  It has been marketed under the title family ba...        1_1             1   \n",
      "4  According to the United Nations Population Fun...        1_1             1   \n",
      "\n",
      "   joint_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "\n",
      "Dimensions of Cleaned Dataset:\n",
      "Rows: 69662, Columns: 6\n",
      "\n",
      "Discarded Dataset:\n",
      "       claim_label                                     topic_sentence  \\\n",
      "30046            0                     Should you block drug addicts?   \n",
      "49989            0  Should parents be allowed to choose the gender...   \n",
      "53753            0          Is social poverty the main cause of crime   \n",
      "55142            0                     Is science a threat to mankind   \n",
      "\n",
      "      claim_candidate_sentence article_id  stance_label  joint_label  \n",
      "30046                      NaN       55_8             1            0  \n",
      "49989                      NaN      96_10             1            0  \n",
      "53753                      NaN     101_10             1            0  \n",
      "55142                      NaN     103_10             1            0  \n",
      "\n",
      "Dimensions of Discarded Dataset:\n",
      "Rows: 4, Columns: 6\n",
      "\n",
      "Files Saved:\n",
      "- Cleaned data with joint labels: cleaned_data_with_joint_labels.csv\n",
      "- Discarded data: discarded_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'all_claims.txt'\n",
    "\n",
    "# Attempt to load the file into a DataFrame with proper handling\n",
    "try:\n",
    "    data = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\"\\t\",  # Assuming tab-separated values\n",
    "        header=None,  # No predefined headers\n",
    "        names=['claim_label', 'topic_sentence', 'claim_candidate_sentence', 'article_id', 'stance_label'],\n",
    "        skipinitialspace=True  # Handle any extra spaces\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define a function to clean and convert stance_label\n",
    "def clean_stance_label(label):\n",
    "    \"\"\"\n",
    "    Convert stance_label -1, 0, 1 to 0, 1, 2 respectively. Return None for invalid values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = int(label)\n",
    "        if value in {-1, 0, 1}:  # Valid stance_label values\n",
    "            return value + 1  # Shift -1, 0, 1 to 0, 1, 2\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return None  # Invalid or non-convertible value\n",
    "\n",
    "# Define a function to clean and convert claim_label\n",
    "def clean_claim_label(label):\n",
    "    \"\"\"\n",
    "    Convert claim_label 'C' to 1 and 'O' to 0. Return None for invalid values.\n",
    "    \"\"\"\n",
    "    if label == 'C':\n",
    "        return 1\n",
    "    elif label == 'O':\n",
    "        return 0\n",
    "    return None  # Invalid or non-convertible value\n",
    "\n",
    "# Define a function to compute joint labels\n",
    "def compute_joint_label(claim_label, stance_label):\n",
    "    \"\"\"\n",
    "    Map claim_label and stance_label to a single joint label:\n",
    "    - Non-Claim -> 0\n",
    "    - Support -> 1\n",
    "    - Contest -> 2\n",
    "    - No Relation -> 3\n",
    "    \"\"\"\n",
    "    if claim_label == 0:  # Non-Claim\n",
    "        return 0\n",
    "    elif claim_label == 1:  # Claim\n",
    "        if stance_label == 1:  # Support\n",
    "            return 1\n",
    "        elif stance_label == 2:  # Contest\n",
    "            return 2\n",
    "        elif stance_label == 0:  # No Relation\n",
    "            return 3\n",
    "    return None  # Invalid combination\n",
    "\n",
    "# Apply validation and save the results\n",
    "try:\n",
    "    # Clean and convert stance_label\n",
    "    data['stance_label'] = data['stance_label'].apply(clean_stance_label)\n",
    "\n",
    "    # Clean and convert claim_label\n",
    "    data['claim_label'] = data['claim_label'].apply(clean_claim_label)\n",
    "\n",
    "    # Compute joint labels\n",
    "    data['joint_label'] = data.apply(\n",
    "        lambda row: compute_joint_label(row['claim_label'], row['stance_label']), axis=1\n",
    "    )\n",
    "\n",
    "    # Identify valid rows with type checks\n",
    "    valid_rows = (\n",
    "        data['stance_label'].notna() &  # Valid stance_label\n",
    "        data['claim_label'].notna() &  # Valid claim_label\n",
    "        data['joint_label'].notna() &  # Valid joint_label\n",
    "        data['topic_sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &  # Valid topic_sentence\n",
    "        data['claim_candidate_sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &  # Valid claim_candidate_sentence\n",
    "        data['article_id'].apply(lambda x: isinstance(x, str) and bool(pd.Series([x]).str.match(r'^\\d+_\\d+$').iloc[0]))  # Valid article_id\n",
    "    )\n",
    "\n",
    "    # Filter valid and invalid rows\n",
    "    cleaned_data = data[valid_rows]\n",
    "    discarded_data = data[~valid_rows]  # Rows that failed validation\n",
    "\n",
    "    # Save cleaned and discarded datasets to files\n",
    "    cleaned_data.to_csv('cleaned_data.csv', index=False)\n",
    "    discarded_data.to_csv('discarded_data.csv', index=False)\n",
    "\n",
    "    # Print summaries\n",
    "    print(\"Unprocessed Dataset:\")\n",
    "    print(data.head())\n",
    "    print(\"\\nDimensions of Unprocessed Dataset:\")\n",
    "    print(f\"Rows: {data.shape[0]}, Columns: {data.shape[1]}\")\n",
    "\n",
    "    print(\"\\nCleaned Dataset:\")\n",
    "    print(cleaned_data.head())\n",
    "    print(\"\\nDimensions of Cleaned Dataset:\")\n",
    "    print(f\"Rows: {cleaned_data.shape[0]}, Columns: {cleaned_data.shape[1]}\")\n",
    "\n",
    "    print(\"\\nDiscarded Dataset:\")\n",
    "    print(discarded_data.head())\n",
    "    print(\"\\nDimensions of Discarded Dataset:\")\n",
    "    print(f\"Rows: {discarded_data.shape[0]}, Columns: {discarded_data.shape[1]}\")\n",
    "\n",
    "    print(\"\\nFiles Saved:\")\n",
    "    print(\"- Cleaned data with joint labels: cleaned_data_with_joint_labels.csv\")\n",
    "    print(\"- Discarded data: discarded_data.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data cleaning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:19:53.404807Z",
     "start_time": "2024-12-01T22:19:52.493595Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, patch_size, emb_size, vocab_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = seq_len // patch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.projection = nn.Linear(emb_size * patch_size, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_len, emb_size]\n",
    "        x = x.unfold(1, self.patch_size, self.patch_size)  # Shape: [batch_size, num_patches, patch_size, emb_size]\n",
    "        x = x.flatten(2)  # Flatten patches: [batch_size, num_patches, emb_size * patch_size]\n",
    "        x = self.projection(x)  # Project to emb_size: [batch_size, num_patches, emb_size]\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(max_len, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding[:x.size(1), :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:19:59.305397Z",
     "start_time": "2024-12-01T22:19:59.299643Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, emb_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x)  # Self-attention\n",
    "        x = x + attn_out  # Residual connection\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)  # Residual connection\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TextVisionTransformer(nn.Module):\n",
    "    def __init__(self, seq_len, patch_size, emb_size, vocab_size, depth, num_heads, ff_dim, max_len=500, num_classes=4):\n",
    "        super(TextVisionTransformer, self).__init__()\n",
    "\n",
    "        # Patch Embedding for each input type\n",
    "        self.topic_patch_embedding = PatchEmbedding(seq_len, patch_size, emb_size, vocab_size)\n",
    "        self.claim_patch_embedding = PatchEmbedding(seq_len, patch_size, emb_size, vocab_size)\n",
    "        self.article_patch_embedding = PatchEmbedding(seq_len, patch_size, emb_size, vocab_size)\n",
    "\n",
    "        # Learnable CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "\n",
    "        # Transformer Encoders\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [TransformerEncoder(emb_size, num_heads, ff_dim) for _ in range(depth)]\n",
    "        )\n",
    "\n",
    "        # Joint Classifier for Multi-Label Classification\n",
    "        self.joint_classifier = nn.Linear(emb_size, num_classes)  # Combined classes: Non-Claim, Support, Contest, No Relation\n",
    "\n",
    "    def forward(self, topic_input_ids, claim_input_ids, article_id_tokens):\n",
    "        \"\"\"\n",
    "        Forward pass through the TextVisionTransformer.\n",
    "        Inputs:\n",
    "        - topic_input_ids: tensor of shape [batch_size, seq_len]\n",
    "        - claim_input_ids: tensor of shape [batch_size, seq_len]\n",
    "        - article_id_tokens: tensor of shape [batch_size, seq_len]\n",
    "        Outputs:\n",
    "        - joint_logits: tensor of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        batch_size = topic_input_ids.size(0)\n",
    "\n",
    "        # Step 1: Patch Embedding for each input\n",
    "        topic_embeddings = self.topic_patch_embedding(topic_input_ids)  # Shape: [batch_size, num_patches, emb_size]\n",
    "        claim_embeddings = self.claim_patch_embedding(claim_input_ids)  # Shape: [batch_size, num_patches, emb_size]\n",
    "        article_embeddings = self.article_patch_embedding(article_id_tokens)  # Shape: [batch_size, num_patches, emb_size]\n",
    "\n",
    "        # Step 2: Concatenate all embeddings\n",
    "        combined_embeddings = torch.cat((topic_embeddings, claim_embeddings, article_embeddings), dim=1)\n",
    "\n",
    "        # Step 3: Add CLS Token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, emb_size]\n",
    "        combined_embeddings = torch.cat((cls_tokens, combined_embeddings), dim=1)  # Shape: [batch_size, total_patches + 1, emb_size]\n",
    "\n",
    "        # Step 4: Transformer Encoder Layers\n",
    "        for encoder in self.encoders:\n",
    "            combined_embeddings = encoder(combined_embeddings)  # Shape remains [batch_size, total_patches + 1, emb_size]\n",
    "\n",
    "        # Step 5: Extract CLS Token\n",
    "        cls_token_final = combined_embeddings[:, 0, :]  # Extract the CLS token: [batch_size, emb_size]\n",
    "\n",
    "        # Step 6: Joint Classifier Output\n",
    "        joint_logits = self.joint_classifier(cls_token_final)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return joint_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:20:06.489284Z",
     "start_time": "2024-12-01T22:20:05.511085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Joint Logits Shape: torch.Size([64, 4])\n",
      "Loss Value: 1.3976267576217651\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model Parameters\n",
    "seq_len = 128         # Input sequence length\n",
    "patch_size = 8        # Number of tokens per patch\n",
    "emb_size = 128        # Size of each embedding vector\n",
    "vocab_size = 30522    # Vocabulary size for embedding (e.g., BERT tokenizer size)\n",
    "depth = 6             # Number of transformer layers\n",
    "num_heads = 8         # Number of attention heads in each layer\n",
    "ff_dim = 512          # Hidden layer size in feed-forward networks\n",
    "num_classes = 4       # Number of joint classes (Non-Claim, Support, Contest, No Relation)\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = TextVisionTransformer(\n",
    "    seq_len=seq_len,          # Sequence length\n",
    "    patch_size=patch_size,    # Patch size\n",
    "    emb_size=emb_size,        # Embedding size\n",
    "    vocab_size=vocab_size,    # Vocabulary size\n",
    "    depth=depth,              # Number of transformer layers\n",
    "    num_heads=num_heads,      # Number of attention heads\n",
    "    ff_dim=ff_dim,            # Feedforward network hidden dimension\n",
    "    num_classes=num_classes   # Number of output classes\n",
    ").to(device)                  # Move the model to the appropriate device (e.g., GPU)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()  # For the joint label\n",
    "\n",
    "# Dummy Input Data\n",
    "batch_size = 64\n",
    "\n",
    "# Simulate three separate input tensors\n",
    "topic_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # Simulated topic input IDs\n",
    "claim_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # Simulated claim input IDs\n",
    "article_id_tokens = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # Simulated article ID tokens\n",
    "\n",
    "# Simulated Joint Labels\n",
    "joint_labels = torch.randint(0, num_classes, (batch_size,)).to(device)  # Simulated joint labels\n",
    "\n",
    "# Forward Pass\n",
    "joint_logits = model(topic_input_ids, claim_input_ids, article_id_tokens)  # Output joint logits\n",
    "\n",
    "# Compute Loss\n",
    "loss = loss_fn(joint_logits, joint_labels)\n",
    "\n",
    "# Debug Outputs\n",
    "print(\"Final Joint Logits Shape:\", joint_logits.shape)  # Expected: [batch_size, num_classes]\n",
    "print(\"Loss Value:\", loss.item())  # Print the computed loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:20:16.329018Z",
     "start_time": "2024-12-01T22:20:15.796333Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"cleaned_data.csv\")  # Load the dataset with joint labels\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=100)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "class CESCData(Dataset):\n",
    "    def __init__(self, data, seq_len, patch_size, vocab_size, device):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.patch_size = patch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device  # Added device argument\n",
    "\n",
    "    def text_to_numeric(self, text):\n",
    "        \"\"\"Simple tokenizer to convert text to numeric values (basic character-level mapping).\"\"\"\n",
    "        return [ord(char) % self.vocab_size for char in text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Process topic_sentence\n",
    "        topic_tokens = self.text_to_numeric(row['topic_sentence'])\n",
    "\n",
    "        # Process claim_candidate_sentence\n",
    "        claim_tokens = self.text_to_numeric(row['claim_candidate_sentence'])\n",
    "\n",
    "        # Padding topic_sentence\n",
    "        topic_padded_length = ((self.seq_len + self.patch_size - 1) // self.patch_size) * self.patch_size\n",
    "        topic_tokens = topic_tokens[:self.seq_len] + [0] * (topic_padded_length - len(topic_tokens))\n",
    "\n",
    "        # Padding claim_candidate_sentence\n",
    "        claim_padded_length = ((self.seq_len + self.patch_size - 1) // self.patch_size) * self.patch_size\n",
    "        claim_tokens = claim_tokens[:self.seq_len] + [0] * (claim_padded_length - len(claim_tokens))\n",
    "\n",
    "        # Process joint label\n",
    "        joint_label = torch.tensor(row['joint_label'], dtype=torch.long).to(self.device)\n",
    "\n",
    "        # Process article_id (optional: tokenize it)\n",
    "        article_id_tokens = self.text_to_numeric(row['article_id'])\n",
    "        article_padded_length = ((self.seq_len + self.patch_size - 1) // self.patch_size) * self.patch_size\n",
    "        article_id_tokens = article_id_tokens[:self.seq_len] + [0] * (article_padded_length - len(article_id_tokens))\n",
    "\n",
    "        return {\n",
    "            'topic_input_ids': torch.tensor(topic_tokens, dtype=torch.long).to(self.device),\n",
    "            'claim_input_ids': torch.tensor(claim_tokens, dtype=torch.long).to(self.device),\n",
    "            'article_id_tokens': torch.tensor(article_id_tokens, dtype=torch.long).to(self.device),\n",
    "            'joint_label': joint_label  # Use the joint label\n",
    "        }\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences for topic, claim, and article ID input IDs\n",
    "    topic_input_ids = pad_sequence([item['topic_input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    claim_input_ids = pad_sequence([item['claim_input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    article_id_tokens = pad_sequence([item['article_id_tokens'] for item in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack joint labels\n",
    "    joint_labels = torch.tensor([item['joint_label'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        'topic_input_ids': topic_input_ids,\n",
    "        'claim_input_ids': claim_input_ids,\n",
    "        'article_id_tokens': article_id_tokens,  # Return tokenized article IDs\n",
    "        'joint_label': joint_labels  # Return joint labels\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = CESCData(train_data, seq_len=seq_len, patch_size=patch_size, vocab_size=vocab_size, device=device)\n",
    "test_dataset = CESCData(test_data, seq_len=seq_len, patch_size=patch_size, vocab_size=vocab_size, device=device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Counter({0: 64772, 2: 2613, 3: 2277})\n",
      "New class distribution after SMOTE:\n",
      "Counter({0: 64772, 2: 20000, 3: 20000})\n",
      "Final class distribution in training data:\n",
      "joint_label\n",
      "0    51772\n",
      "2    16025\n",
      "3    16020\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"cleaned_data.csv\")  # Ensure 'topic_sentence', 'claim_candidate_sentence', 'article_id', and 'joint_label' are present\n",
    "\n",
    "# Convert columns to string to avoid float-related issues\n",
    "data[\"topic_sentence\"] = data[\"topic_sentence\"].astype(str)\n",
    "data[\"claim_candidate_sentence\"] = data[\"claim_candidate_sentence\"].astype(str)\n",
    "data[\"article_id\"] = data[\"article_id\"].astype(str)\n",
    "\n",
    "# Combine text features for TF-IDF transformation\n",
    "data[\"combined_text\"] = data[\"topic_sentence\"] + \" \" + data[\"claim_candidate_sentence\"]\n",
    "\n",
    "# Extract text and target labels\n",
    "X_text = data[\"combined_text\"]  # Combined text column\n",
    "y = data[\"joint_label\"]         # Target column\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Use up to 5000 features for efficiency\n",
    "X_tfidf = tfidf.fit_transform(X_text)\n",
    "\n",
    "# Check original class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(Counter(y))\n",
    "\n",
    "# Define adjusted sampling strategy for SMOTE\n",
    "# Smaller sample sizes for oversampling to reduce dataset size\n",
    "sampling_strategy = {\n",
    "    0: 64772,  # Keep Non-Claim size unchanged\n",
    "    2: 20000,  # Increase Support to 20,000\n",
    "    3: 20000   # Increase No Relation to 20,000\n",
    "}\n",
    "\n",
    "# Apply SMOTE with adjusted sampling strategy\n",
    "smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"New class distribution after SMOTE:\")\n",
    "print(Counter(y_resampled))\n",
    "\n",
    "# Combine resampled data into a DataFrame\n",
    "resampled_data = pd.DataFrame(X_resampled.toarray(), columns=tfidf.get_feature_names_out())\n",
    "resampled_data[\"joint_label\"] = y_resampled\n",
    "\n",
    "# Assign article_id and sentences back for interpretability\n",
    "resampled_data[\"article_id\"] = data[\"article_id\"][:len(y_resampled)].reset_index(drop=True)  # Retain article IDs\n",
    "resampled_data[\"topic_sentence\"] = data[\"topic_sentence\"][:len(y_resampled)].reset_index(drop=True)\n",
    "resampled_data[\"claim_candidate_sentence\"] = data[\"claim_candidate_sentence\"][:len(y_resampled)].reset_index(drop=True)\n",
    "\n",
    "# Split the resampled data into train and test sets\n",
    "train_data, test_data = train_test_split(resampled_data, test_size=0.2, random_state=100)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Print final class distribution\n",
    "print(\"Final class distribution in training data:\")\n",
    "print(train_data[\"joint_label\"].value_counts())\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = CESCData(train_data, seq_len=seq_len, patch_size=patch_size, vocab_size=vocab_size, device=device)\n",
    "test_dataset = CESCData(test_data, seq_len=seq_len, patch_size=patch_size, vocab_size=vocab_size, device=device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:20:24.954729Z",
     "start_time": "2024-12-01T22:20:24.950447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Before training starts\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_model(model, train_loader, optimizer, scheduler, loss_fn, epochs=3):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Extract inputs and labels from the batch\n",
    "            topic_input_ids = batch['topic_input_ids'].to(device)\n",
    "            claim_input_ids = batch['claim_input_ids'].to(device)\n",
    "            article_id_tokens = batch['article_id_tokens'].to(device)\n",
    "            joint_labels = batch['joint_label'].to(device)  # Directly use the joint label\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            joint_logits = model(topic_input_ids, claim_input_ids, article_id_tokens)  # Single joint output\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(joint_logits, joint_labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch loss and learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, LR: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#print(f\"Model is on device: {next(model.parameters()).device}\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_model(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m----> 6\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m,\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      9\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),  \u001b[38;5;66;03m# Single loss function for joint labels\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "#print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=nn.CrossEntropyLoss(),  # Single loss function for joint labels\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset using the joint label.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # Extract inputs and labels from the batch\n",
    "            topic_input_ids = batch['topic_input_ids'].to(device)\n",
    "            claim_input_ids = batch['claim_input_ids'].to(device)\n",
    "            article_id_tokens = batch['article_id_tokens'].to(device)\n",
    "            joint_labels = batch['joint_label'].to(device)  # Directly use the joint label\n",
    "\n",
    "            # Forward pass\n",
    "            joint_logits = model(topic_input_ids, claim_input_ids, article_id_tokens)\n",
    "\n",
    "            # Predictions\n",
    "            preds = torch.argmax(joint_logits, dim=1).cpu().numpy()  # Get predicted class indices\n",
    "            labels = joint_labels.cpu().numpy()  # Get ground truth labels\n",
    "\n",
    "            # Append predictions and labels for evaluation\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=[\"Non-Claim\", \"Support\", \"Contest\", \"No Relation\"]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n",
      "\n",
      "Claim Confusion Matrix:\n",
      "[[12956     0]\n",
      " [  977     0]]\n",
      "\n",
      "Stance Confusion Matrix:\n",
      "[[    0   454     0]\n",
      " [    0 12956     0]\n",
      " [    0   523     0]]\n",
      "Claim Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Claim       0.93      1.00      0.96     12956\n",
      "       Claim       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.93     13933\n",
      "   macro avg       0.46      0.50      0.48     13933\n",
      "weighted avg       0.86      0.93      0.90     13933\n",
      "\n",
      "Stance Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Support       0.00      0.00      0.00       454\n",
      "     Contest       0.93      1.00      0.96     12956\n",
      " No Relation       0.00      0.00      0.00       523\n",
      "\n",
      "    accuracy                           0.93     13933\n",
      "   macro avg       0.31      0.33      0.32     13933\n",
      "weighted avg       0.86      0.93      0.90     13933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\n",
    "    model=model,                # Use the same model trained in train_model\n",
    "    test_loader=test_loader     # Pass the test DataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?: True\n",
      "Device name: NVIDIA RTX A5000\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/elx12/nlp/myenv/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_460740/2039698867.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Initializing global attention on CLS token...\n",
      "/home/elx12/nlp/myenv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='344' max='10986' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  344/10986 08:34 < 4:26:35, 0.67 it/s, Epoch 0.06/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    LongformerForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "# Optimized Dataset class for Longformer\n",
    "class LongformerDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.topic_sentences = data[\"topic_sentence\"].tolist()\n",
    "        self.claim_candidate_sentences = data[\"claim_candidate_sentence\"].tolist()\n",
    "        self.labels = data[\"joint_label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        topic_sentence = str(self.topic_sentences[idx])\n",
    "        claim_candidate_sentence = str(self.claim_candidate_sentences[idx])\n",
    "        label = int(self.labels[idx])  # Ensure label is an integer, not Tensor\n",
    "\n",
    "        # Tokenize input sequences\n",
    "        inputs = self.tokenizer(\n",
    "            topic_sentence,\n",
    "            claim_candidate_sentence,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        inputs[\"labels\"] = label  # Add label directly to the dictionary\n",
    "        return inputs\n",
    "\n",
    "# Prepare datasets\n",
    "max_length = 512  # Adjust based on your data and memory\n",
    "train_dataset = LongformerDataset(train_data, tokenizer, max_length)\n",
    "test_dataset = LongformerDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "    report = classification_report(\n",
    "        labels, preds, zero_division=0, output_dict=True\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "# Load Longformer model\n",
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\", num_labels=4\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=12,  # Increased batch size\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=1,  # Simplify training\n",
    "    num_train_epochs=5,  # Start with 1 epoch\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1000,  # Log less frequently\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=True,  # Mixed precision for faster training\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
