{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d4cd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_cesc(file_path):\n",
    "    labels = []\n",
    "    try:\n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep='\\t',  # Adjust this to ' ' if the file is space-separated\n",
    "            header=None,  # No header row in the file\n",
    "            quoting=csv.QUOTE_NONE,  # Handle special characters\n",
    "            engine='python',  # More forgiving parser\n",
    "            on_bad_lines='skip'  # Skip lines with unexpected formatting\n",
    "        )\n",
    "        # Assign column names\n",
    "        df.columns = ['claim_labels', 'topic_setence', 'claim_candidate', 'id', 'labels']\n",
    "        claim_labels = df['claim_labels'].tolist()\n",
    "        labels = df['labels'].tolist()\n",
    "        return labels, claim_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a04c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import collections\n",
    "\n",
    "def most_frequent_class_baseline_cesc(train_file, test_file):\n",
    "    # Load labels and claim labels\n",
    "    train_labels, train_claim_labels = load_labels_cesc(train_file)\n",
    "    test_labels, test_claim_labels = load_labels_cesc(test_file)\n",
    "    \n",
    "    # Find the most frequent class in the training labels\n",
    "    label_counts = collections.Counter(train_labels)\n",
    "    most_frequent_label = label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Find the most frequent class in the training claim labels\n",
    "    claim_label_counts = collections.Counter(train_claim_labels)\n",
    "    most_frequent_claim_label = claim_label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    print(f\"Most frequent label in training data: {most_frequent_label}\")\n",
    "    print(f\"Most frequent claim label in training data: {most_frequent_claim_label}\")\n",
    "    \n",
    "    # Predict the most frequent class for all test labels and claim labels\n",
    "    predictions = [most_frequent_label] * len(test_labels)\n",
    "    claim_predictions = [most_frequent_claim_label] * len(test_claim_labels)\n",
    "    \n",
    "    # Evaluate accuracy for labels\n",
    "    accuracy = sum(1 for pred, true in zip(predictions, test_labels) if pred == true) / len(test_labels)\n",
    "    print(f\"Accuracy (stance labels): {accuracy:.2f}\")\n",
    "    \n",
    "    # Evaluate accuracy for claim labels\n",
    "    claim_accuracy = sum(1 for pred, true in zip(claim_predictions, test_claim_labels) if pred == true) / len(test_claim_labels)\n",
    "    print(f\"Accuracy (claim labels): {claim_accuracy:.2f}\")\n",
    "    \n",
    "    # Generate and print classification reports\n",
    "    label_report = classification_report(test_labels, predictions, zero_division=0)\n",
    "    claim_label_report = classification_report(test_claim_labels, claim_predictions, zero_division=0)\n",
    "    \n",
    "    print(\"\\nClassification Report (stance labels):\")\n",
    "    print(label_report)\n",
    "    \n",
    "    print(\"\\nClassification Report (claim labels):\")\n",
    "    print(claim_label_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cffa185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to calculate random performance baseline\n",
    "def random_baseline_cesc(train_file, test_file):\n",
    "    # Load labels for stance and claims\n",
    "    train_stance_labels, train_claim_labels = load_labels_cesc(train_file)\n",
    "    test_stance_labels, test_claim_labels = load_labels_cesc(test_file)\n",
    "    \n",
    "    # Get unique stance labels and their frequencies from the training set\n",
    "    stance_label_counts = collections.Counter(train_stance_labels)\n",
    "    stance_labels = list(stance_label_counts.keys())\n",
    "    stance_label_probabilities = [stance_label_counts[label] / sum(stance_label_counts.values()) for label in stance_labels]\n",
    "    \n",
    "    print(f\"Stance Labels: {stance_labels}\")\n",
    "    print(f\"Stance Probabilities: {stance_label_probabilities}\")\n",
    "    \n",
    "    # Predict random stance labels for the test set\n",
    "    stance_predictions = random.choices(stance_labels, weights=stance_label_probabilities, k=len(test_stance_labels))\n",
    "    \n",
    "    # Evaluate stance accuracy and classification report\n",
    "    stance_accuracy = sum(1 for pred, true in zip(stance_predictions, test_stance_labels) if pred == true) / len(test_stance_labels)\n",
    "    print(f\"\\nStance Accuracy: {stance_accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report (stance labels):\")\n",
    "    print(classification_report(test_stance_labels, stance_predictions, zero_division=0))\n",
    "    \n",
    "    # Get unique claim labels and their frequencies from the training set\n",
    "    claim_label_counts = collections.Counter(train_claim_labels)\n",
    "    claim_labels = list(claim_label_counts.keys())\n",
    "    claim_label_probabilities = [claim_label_counts[label] / sum(claim_label_counts.values()) for label in claim_labels]\n",
    "    \n",
    "    print(f\"\\nClaim Labels: {claim_labels}\")\n",
    "    print(f\"Claim Probabilities: {claim_label_probabilities}\")\n",
    "    \n",
    "    # Predict random claim labels for the test set\n",
    "    claim_predictions = random.choices(claim_labels, weights=claim_label_probabilities, k=len(test_claim_labels))\n",
    "    \n",
    "    # Evaluate claim accuracy and classification report\n",
    "    claim_accuracy = sum(1 for pred, true in zip(claim_predictions, test_claim_labels) if pred == true) / len(test_claim_labels)\n",
    "    print(f\"\\nClaim Accuracy: {claim_accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report (claim labels):\")\n",
    "    print(classification_report(test_claim_labels, claim_predictions, zero_division=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1cd4290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent label in training data: 0\n",
      "Most frequent claim label in training data: O\n",
      "Accuracy (stance labels): 0.93\n",
      "Accuracy (claim labels): 0.93\n",
      "\n",
      "Classification Report (stance labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       271\n",
      "           0       0.93      1.00      0.96      6538\n",
      "           1       0.00      0.00      0.00       256\n",
      "\n",
      "    accuracy                           0.93      7065\n",
      "   macro avg       0.31      0.33      0.32      7065\n",
      "weighted avg       0.86      0.93      0.89      7065\n",
      "\n",
      "\n",
      "Classification Report (claim labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.00      0.00      0.00       527\n",
      "           O       0.93      1.00      0.96      6538\n",
      "\n",
      "    accuracy                           0.93      7065\n",
      "   macro avg       0.46      0.50      0.48      7065\n",
      "weighted avg       0.86      0.93      0.89      7065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_frequent_class_baseline_cesc('/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/claims/train.txt', '/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/claims/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2d52da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance Labels: [0, -1, 1]\n",
      "Stance Probabilities: [0.9303075039608238, 0.031920639493014547, 0.0377718565461616]\n",
      "\n",
      "Stance Accuracy: 0.87\n",
      "\n",
      "Classification Report (stance labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.04      0.04      0.04       271\n",
      "           0       0.92      0.93      0.93      6538\n",
      "           1       0.04      0.04      0.04       256\n",
      "\n",
      "    accuracy                           0.87      7065\n",
      "   macro avg       0.34      0.34      0.34      7065\n",
      "weighted avg       0.86      0.87      0.86      7065\n",
      "\n",
      "\n",
      "Claim Labels: ['O', 'C']\n",
      "Claim Probabilities: [0.9303075039608238, 0.06969249603917615]\n",
      "\n",
      "Claim Accuracy: 0.87\n",
      "\n",
      "Classification Report (claim labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.07      0.06      0.07       527\n",
      "           O       0.93      0.93      0.93      6538\n",
      "\n",
      "    accuracy                           0.87      7065\n",
      "   macro avg       0.50      0.50      0.50      7065\n",
      "weighted avg       0.86      0.87      0.86      7065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_baseline_cesc('/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/claims/train.txt', '/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/claims/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "573c7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_cepe(file_path):\n",
    "    claim_labels = []\n",
    "    evidence_labels = []\n",
    "    try:\n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep='\\t',  # Adjust this to ' ' if the file is space-separated\n",
    "            header=None,  # No header row in the file\n",
    "            quoting=csv.QUOTE_NONE,  # Handle special characters\n",
    "            engine='python',  # More forgiving parser\n",
    "            on_bad_lines='skip'  # Skip lines with unexpected formatting\n",
    "        )\n",
    "        # Assign column names\n",
    "        df.columns = ['claim_labels', 'topic_Sentence', 'evidence_label', 'claim_sentence', 'evidence_candidate_sentence', 'article_id', 'full_label']\n",
    "        \n",
    "        # Extract the claim_labels and labels columns\n",
    "        claim_labels = df['claim_labels'].tolist()\n",
    "        evidence_labels = df['evidence_label'].tolist()\n",
    "        print(f\"num rows: {len(df)}\")\n",
    "        return claim_labels, evidence_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6ea6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def most_frequent_class_baseline_cepe(train_file, test_file):\n",
    "# Load claim and evidence labels\n",
    "    train_claim_labels, train_evidence_labels = load_labels_cepe(train_file)\n",
    "    test_claim_labels, test_evidence_labels = load_labels_cepe(test_file)\n",
    "\n",
    "    # Get the most frequent labels\n",
    "    most_frequent_claim_label = collections.Counter(train_claim_labels).most_common(1)[0][0]\n",
    "    most_frequent_evidence_label = collections.Counter(train_evidence_labels).most_common(1)[0][0]\n",
    "\n",
    "    print(f\"Most frequent claim label: {most_frequent_claim_label}\")\n",
    "    print(f\"Most frequent evidence label: {most_frequent_evidence_label}\")\n",
    "\n",
    "    # Predict the most frequent class for all test instances\n",
    "    claim_predictions = [most_frequent_claim_label] * len(test_claim_labels)\n",
    "    evidence_predictions = [most_frequent_evidence_label] * len(test_evidence_labels)\n",
    "\n",
    "    # Combine claim and evidence labels into single string labels\n",
    "    true_labels = [f\"{c}_{e}\" for c, e in zip(test_claim_labels, test_evidence_labels)]\n",
    "    predicted_labels = [f\"{c}_{e}\" for c, e in zip(claim_predictions, evidence_predictions)]\n",
    "\n",
    "    # Calculate and display the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b208cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_cepe(train_file, test_file):\n",
    "    # Load claim and evidence labels\n",
    "    train_claim_labels, train_evidence_labels = load_labels_cepe(train_file)\n",
    "    test_claim_labels, test_evidence_labels = load_labels_cepe(test_file)\n",
    "    \n",
    "    # Get unique labels and their probabilities from the training set\n",
    "    claim_label_counts = collections.Counter(train_claim_labels)\n",
    "    evidence_label_counts = collections.Counter(train_evidence_labels)\n",
    "    \n",
    "    claim_labels = list(claim_label_counts.keys())\n",
    "    claim_probabilities = [claim_label_counts[label] / sum(claim_label_counts.values()) for label in claim_labels]\n",
    "    \n",
    "    evidence_labels = list(evidence_label_counts.keys())\n",
    "    evidence_probabilities = [evidence_label_counts[label] / sum(evidence_label_counts.values()) for label in evidence_labels]\n",
    "    \n",
    "    print(f\"Claim Labels: {claim_labels}\")\n",
    "    print(f\"Claim Probabilities: {claim_probabilities}\")\n",
    "    print(f\"Evidence Labels: {evidence_labels}\")\n",
    "    print(f\"Evidence Probabilities: {evidence_probabilities}\")\n",
    "    \n",
    "    # Predict random labels for the test set\n",
    "    claim_predictions = random.choices(claim_labels, weights=claim_probabilities, k=len(test_claim_labels))\n",
    "    evidence_predictions = random.choices(evidence_labels, weights=evidence_probabilities, k=len(test_evidence_labels))\n",
    "    \n",
    "    # Combine claim and evidence labels into single string labels\n",
    "    true_labels = [f\"{c}_{e}\" for c, e in zip(test_claim_labels, test_evidence_labels)]\n",
    "    true_train_labels = [f\"{c}_{e}\" for c, e in zip(train_claim_labels, train_evidence_labels)]\n",
    "    predicted_labels = [f\"{c}_{e}\" for c, e in zip(claim_predictions, evidence_predictions)]\n",
    "    \n",
    "        # Count the occurrences of each class\n",
    "    class_counts = Counter(true_train_labels)\n",
    "\n",
    "    # Print the number of instances for each class\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"Class: {label}, Instances: {count}\")\n",
    "\n",
    "    # Evaluate using sklearn's classification_report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    unique_labels = list(set(true_labels))  # Get unique combinations of claim and evidence labels\n",
    "    print(classification_report(true_labels, predicted_labels, labels=unique_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e66e74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows: 57398\n",
      "num rows: 8172\n",
      "Most frequent claim label: O\n",
      "Most frequent evidence label: O\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C_E       0.00      0.00      0.00       135\n",
      "         C_O       0.00      0.00      0.00       240\n",
      "         O_E       0.00      0.00      0.00       973\n",
      "         O_O       0.84      1.00      0.91      6824\n",
      "\n",
      "    accuracy                           0.84      8172\n",
      "   macro avg       0.21      0.25      0.23      8172\n",
      "weighted avg       0.70      0.84      0.76      8172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_frequent_class_baseline_cepe('/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/CEPE/train.txt', '/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/CEPE/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03ff19b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows: 57398\n",
      "num rows: 8172\n",
      "Claim Labels: ['O', 'C']\n",
      "Claim Probabilities: [0.9544757657061221, 0.045524234293877835]\n",
      "Evidence Labels: ['O', 'E']\n",
      "Evidence Probabilities: [0.8732011568347329, 0.12679884316526707]\n",
      "Class: O_O, Instances: 48513\n",
      "Class: C_E, Instances: 1006\n",
      "Class: C_O, Instances: 1607\n",
      "Class: O_E, Instances: 6272\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         O_O       0.83      0.83      0.83      6824\n",
      "         C_O       0.03      0.04      0.03       240\n",
      "         C_E       0.00      0.00      0.00       135\n",
      "         O_E       0.12      0.12      0.12       973\n",
      "\n",
      "    accuracy                           0.71      8172\n",
      "   macro avg       0.24      0.25      0.25      8172\n",
      "weighted avg       0.71      0.71      0.71      8172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_baseline_cepe('/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/CEPE/train.txt', '/Users/swarnachakraborty/Study_Materials/phd/Phd/Fall24/cs7389J/Project/IAM-main/CEPE/test.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
