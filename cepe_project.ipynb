{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuoG37z81aI_",
        "outputId": "5f2a8277-e42d-416f-b7ac-18904d37f3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed Dataset Summary:\n",
            "Total Rows: 57398\n",
            "Valid Rows: 57391\n",
            "Discarded Rows: 7\n",
            "\n",
            "Sample of Cleaned Data:\n",
            "   claim_label                          topic_sentence  evidence_label  \\\n",
            "0            0  Should we abandon the one-child policy               0   \n",
            "1            0  Should we abandon the one-child policy               0   \n",
            "2            0  Should we abandon the one-child policy               0   \n",
            "3            0  Should we abandon the one-child policy               0   \n",
            "4            0  Should we abandon the one-child policy               0   \n",
            "\n",
            "                                      claim_sentence  \\\n",
            "0  A 2009 study at the University of Ulster found...   \n",
            "1  A 2009 study at the University of Ulster found...   \n",
            "2  A 2009 study at the University of Ulster found...   \n",
            "3  A 2009 study at the University of Ulster found...   \n",
            "4  A 2009 study at the University of Ulster found...   \n",
            "\n",
            "                         evidence_candidate_sentence article_id full_label  \n",
            "0  Low fertility which increases the need for sex...        1_1          O  \n",
            "1  The United Nations Population Fund states that...        1_1          O  \n",
            "2                        Preference for sex of child        1_1          O  \n",
            "3  In many cultures, male offspring are desired i...        1_1          O  \n",
            "4  In countries such as India, China, Indonesia a...        1_1          O  \n",
            "\n",
            "Sample of Discarded Data:\n",
            "       claim_label                          topic_sentence  evidence_label  \\\n",
            "212              0  Should we abandon the one-child policy               0   \n",
            "233              0  Should we abandon the one-child policy               0   \n",
            "253              0  Should we abandon the one-child policy               1   \n",
            "34229            0         Should we support animal rights               0   \n",
            "44081            0          Is science a threat to mankind               0   \n",
            "\n",
            "                                          claim_sentence  \\\n",
            "212    In October 2015, the Chinese news agency Xinhu...   \n",
            "233    They have this huge crushing demographic crisi...   \n",
            "253    Since the citizens of China are living longer ...   \n",
            "34229  In Christian theology, the founder of the Meth...   \n",
            "44081  In his Industrial Society and Its Future also ...   \n",
            "\n",
            "                             evidence_candidate_sentence article_id full_label  \n",
            "212    That could put immense pressure on the economy...        1_7       None  \n",
            "233    That could put immense pressure on the economy...        1_7       None  \n",
            "253    That could put immense pressure on the economy...        1_7       None  \n",
            "34229  Cats do not have any interest in voting, so th...       74_5       None  \n",
            "44081                                                NaN     103_10          O  \n",
            "\n",
            "Files Saved:\n",
            "- Cleaned data: cleaned_cepe_data.csv\n",
            "- Discarded data: discarded_cepe_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"cepe_dataset.txt\"  # Replace with the actual file path\n",
        "try:\n",
        "    data = pd.read_csv(\n",
        "        file_path,\n",
        "        delimiter=\"\\t\",  # Assuming tab-separated values\n",
        "        header=None,  # No predefined headers\n",
        "        names=[\n",
        "            'claim_label', 'topic_sentence', 'evidence_label',\n",
        "            'claim_sentence', 'evidence_candidate_sentence', 'article_id', 'full_label'],\n",
        "        skipinitialspace=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Define helper functions\n",
        "def clean_claim_label(label):\n",
        "    \"\"\"\n",
        "    Validate and map claim labels.\n",
        "    'C' -> 1 (claim), 'O' -> 0 (non-claim)\n",
        "    \"\"\"\n",
        "    return 1 if label == 'C' else 0 if label == 'O' else None\n",
        "\n",
        "def clean_evidence_label(label):\n",
        "    \"\"\"\n",
        "    Validate and map evidence labels.\n",
        "    'E' -> 1 (evidence), 'O' -> 0 (non-evidence)\n",
        "    \"\"\"\n",
        "    return 1 if label == 'E' else 0 if label == 'O' else None\n",
        "\n",
        "def validate_full_label(label):\n",
        "    \"\"\"\n",
        "    Validate the format of full_label (e.g., 'C-index', 'E-B-index', 'E-I-index', 'O').\n",
        "    \"\"\"\n",
        "    if label == 'O':\n",
        "        return label\n",
        "    if isinstance(label, str) and all(\n",
        "        part.startswith(('C-', 'E-B-', 'E-I-')) or part == 'O'\n",
        "        for part in label.split('|')\n",
        "    ):\n",
        "        return label\n",
        "    return None\n",
        "\n",
        "def validate_article_id(article_id):\n",
        "    \"\"\"\n",
        "    Validate article_id format as 'number_number' (e.g., '1_3').\n",
        "    \"\"\"\n",
        "    return isinstance(article_id, str) and bool(pd.Series([article_id]).str.match(r'^\\d+_\\d+$').iloc[0])\n",
        "\n",
        "# Clean and validate data\n",
        "try:\n",
        "    data['claim_label'] = data['claim_label'].apply(clean_claim_label)\n",
        "    data['evidence_label'] = data['evidence_label'].apply(clean_evidence_label)\n",
        "    data['full_label'] = data['full_label'].apply(validate_full_label)\n",
        "\n",
        "    valid_rows = (\n",
        "        data['claim_label'].notna() &\n",
        "        data['evidence_label'].notna() &\n",
        "        data['full_label'].notna() &\n",
        "        data['topic_sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
        "        data['claim_sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
        "        data['evidence_candidate_sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
        "        data['article_id'].apply(validate_article_id)\n",
        "    )\n",
        "\n",
        "    # Separate valid and invalid rows\n",
        "    cleaned_data = data[valid_rows]\n",
        "    discarded_data = data[~valid_rows]\n",
        "\n",
        "    # Save processed data\n",
        "    cleaned_data.to_csv(\"cleaned_cepe_data.csv\", index=False)\n",
        "    discarded_data.to_csv(\"discarded_cepe_data.csv\", index=False)\n",
        "\n",
        "    # Summarize results\n",
        "    print(\"Processed Dataset Summary:\")\n",
        "    print(f\"Total Rows: {data.shape[0]}\")\n",
        "    print(f\"Valid Rows: {cleaned_data.shape[0]}\")\n",
        "    print(f\"Discarded Rows: {discarded_data.shape[0]}\")\n",
        "    print(\"\\nSample of Cleaned Data:\")\n",
        "    print(cleaned_data.head())\n",
        "    print(\"\\nSample of Discarded Data:\")\n",
        "    print(discarded_data.head())\n",
        "\n",
        "    print(\"\\nFiles Saved:\")\n",
        "    print(\"- Cleaned data: cleaned_cepe_data.csv\")\n",
        "    print(\"- Discarded data: discarded_cepe_data.csv\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during preprocessing: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01b0jYoc3eiI",
        "outputId": "21c9d890-d83a-488d-8298-3e3d58d7b2fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated dataset with 'combined_label' column added.\n",
            "   claim_label                          topic_sentence  evidence_label  \\\n",
            "0            0  Should we abandon the one-child policy               0   \n",
            "1            0  Should we abandon the one-child policy               0   \n",
            "2            0  Should we abandon the one-child policy               0   \n",
            "3            0  Should we abandon the one-child policy               0   \n",
            "4            0  Should we abandon the one-child policy               0   \n",
            "\n",
            "                                      claim_sentence  \\\n",
            "0  A 2009 study at the University of Ulster found...   \n",
            "1  A 2009 study at the University of Ulster found...   \n",
            "2  A 2009 study at the University of Ulster found...   \n",
            "3  A 2009 study at the University of Ulster found...   \n",
            "4  A 2009 study at the University of Ulster found...   \n",
            "\n",
            "                         evidence_candidate_sentence article_id full_label  \\\n",
            "0  Low fertility which increases the need for sex...        1_1          O   \n",
            "1  The United Nations Population Fund states that...        1_1          O   \n",
            "2                        Preference for sex of child        1_1          O   \n",
            "3  In many cultures, male offspring are desired i...        1_1          O   \n",
            "4  In countries such as India, China, Indonesia a...        1_1          O   \n",
            "\n",
            "  combined_label  \n",
            "0            0_0  \n",
            "1            0_0  \n",
            "2            0_0  \n",
            "3            0_0  \n",
            "4            0_0  \n",
            "\n",
            "Updated dataset saved as: updated_cleaned_cepe_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned dataset\n",
        "file_path = 'cleaned_cepe_data.csv'  # Replace with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Add a new column 'combined_label' which concatenates 'claim_label' and 'evidence_label'\n",
        "try:\n",
        "    data['combined_label'] = data['claim_label'].astype(str) + \"_\" + data['evidence_label'].astype(str)\n",
        "\n",
        "    # Save the updated dataset\n",
        "    output_file = 'updated_cleaned_cepe_data.csv'  # Update the file name/path as needed\n",
        "    data.to_csv(output_file, index=False)\n",
        "\n",
        "    # Display a summary and first few rows of the updated dataset\n",
        "    print(\"Updated dataset with 'combined_label' column added.\")\n",
        "    print(data.head())\n",
        "\n",
        "    print(f\"\\nUpdated dataset saved as: {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while adding the combined_label column: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated dataset with 'encoded_label' column added.\n",
            "  combined_label  encoded_label\n",
            "0            0_0              0\n",
            "1            0_0              0\n",
            "2            0_0              0\n",
            "3            0_0              0\n",
            "4            0_0              0\n",
            "\n",
            "Updated dataset saved as: encoded_cleaned_cepe_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated dataset\n",
        "file_path = 'updated_cleaned_cepe_data.csv'  # Replace with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the label encoding mapping\n",
        "label_encoding = {\n",
        "    '0_0': 0,\n",
        "    '0_1': 1,\n",
        "    '1_0': 2,\n",
        "    '1_1': 3\n",
        "}\n",
        "\n",
        "# Add a new column 'encoded_label' with encoded values\n",
        "try:\n",
        "    data['encoded_label'] = data['combined_label'].map(label_encoding)\n",
        "    \n",
        "    # Save the updated dataset with encoded labels\n",
        "    output_file = 'encoded_cleaned_cepe_data.csv'  # Update the file name/path as needed\n",
        "    data.to_csv(output_file, index=False)\n",
        "    \n",
        "    # Display a summary and first few rows of the updated dataset\n",
        "    print(\"Updated dataset with 'encoded_label' column added.\")\n",
        "    print(data[['combined_label', 'encoded_label']].head())\n",
        "\n",
        "    print(f\"\\nUpdated dataset saved as: {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while encoding the combined_label column: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kurt4fkg1i2b",
        "outputId": "09108d51-7d57-4cfd-fa49-832129276724"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load and Clean Dataset\n",
        "file_path = \"encoded_cleaned_cepe_data.csv\"  # Path to your cleaned dataset with encoded labels\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(f\"Loaded dataset with {len(data)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Dataset Class\n",
        "class CEPEDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, patch_size, vocab_size):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.patch_size = patch_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def text_to_numeric(self, text):\n",
        "        \"\"\"Convert text to numeric values (character-level mapping).\"\"\"\n",
        "        return [ord(char) % self.vocab_size for char in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Tokenize claims, evidence, and topics\n",
        "        claim_tokens = self.text_to_numeric(row['claim_sentence'])\n",
        "        evidence_tokens = self.text_to_numeric(row['evidence_candidate_sentence'])\n",
        "        topic_tokens = self.text_to_numeric(row['topic_sentence'])\n",
        "\n",
        "        # Pad to sequence length\n",
        "        padded_length = ((self.seq_len + self.patch_size - 1) // self.patch_size) * self.patch_size\n",
        "        claim_tokens = claim_tokens[:self.seq_len] + [0] * (padded_length - len(claim_tokens))\n",
        "        evidence_tokens = evidence_tokens[:self.seq_len] + [0] * (padded_length - len(evidence_tokens))\n",
        "        topic_tokens = topic_tokens[:self.seq_len] + [0] * (padded_length - len(topic_tokens))\n",
        "\n",
        "        # Extract encoded label\n",
        "        encoded_label = torch.tensor(row['encoded_label'], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'claim_input_ids': torch.tensor(claim_tokens, dtype=torch.long),\n",
        "            'evidence_input_ids': torch.tensor(evidence_tokens, dtype=torch.long),\n",
        "            'topic_input_ids': torch.tensor(topic_tokens, dtype=torch.long),\n",
        "            'encoded_label': encoded_label\n",
        "        }\n",
        "\n",
        "# Collate Function\n",
        "def cepe_collate_fn(batch):\n",
        "    claim_input_ids = pad_sequence([item['claim_input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
        "    evidence_input_ids = pad_sequence([item['evidence_input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
        "    topic_input_ids = pad_sequence([item['topic_input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
        "    encoded_labels = torch.tensor([item['encoded_label'] for item in batch], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'claim_input_ids': claim_input_ids,\n",
        "        'evidence_input_ids': evidence_input_ids,\n",
        "        'topic_input_ids': topic_input_ids,\n",
        "        'encoded_label': encoded_labels\n",
        "    }\n",
        "\n",
        "# DataLoader\n",
        "seq_len = 128\n",
        "patch_size = 8\n",
        "vocab_size = 30522\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = CEPEDataset(data, seq_len=seq_len, patch_size=patch_size, vocab_size=vocab_size)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=cepe_collate_fn)\n",
        "\n",
        "# Model\n",
        "class TextVisionTransformerCEPE(nn.Module):\n",
        "    def __init__(self, seq_len, patch_size, emb_size, vocab_size, depth, num_heads, ff_dim):\n",
        "        super(TextVisionTransformerCEPE, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=emb_size, nhead=num_heads, dim_feedforward=ff_dim, batch_first=True),\n",
        "            num_layers=depth\n",
        "        )\n",
        "        self.projection = nn.Linear(emb_size * 3, emb_size)  # Project concatenated embeddings back to emb_size\n",
        "        self.classifier = nn.Linear(emb_size, 4)  # Multi-class classifier (4 classes for encoded_label)\n",
        "\n",
        "    def forward(self, claim_input_ids, evidence_input_ids, topic_input_ids):\n",
        "        # Embed claims, evidence, and topics\n",
        "        claim_emb = self.embedding(claim_input_ids).mean(dim=1)\n",
        "        evidence_emb = self.embedding(evidence_input_ids).mean(dim=1)\n",
        "        topic_emb = self.embedding(topic_input_ids).mean(dim=1)\n",
        "\n",
        "        # Concatenate embeddings and project back to emb_size\n",
        "        combined = torch.cat((claim_emb, evidence_emb, topic_emb), dim=1)  # [batch_size, emb_size * 3]\n",
        "        combined = self.projection(combined)  # [batch_size, emb_size]\n",
        "\n",
        "        # Pass through the transformer encoder\n",
        "        encoded = self.encoder(combined.unsqueeze(1))  # [batch_size, 1, emb_size]\n",
        "        encoded = encoded.squeeze(1)  # [batch_size, emb_size]\n",
        "\n",
        "        # Classify the encoded representation\n",
        "        logits = self.classifier(encoded)  # [batch_size, 4]\n",
        "        return logits\n",
        "\n",
        "# Instantiate Model\n",
        "model = TextVisionTransformerCEPE(\n",
        "    seq_len=seq_len, patch_size=patch_size, emb_size=128, vocab_size=vocab_size,\n",
        "    depth=6, num_heads=8, ff_dim=512\n",
        ")\n",
        "\n",
        "# Training and Evaluation\n",
        "def train_model(model, data_loader, optimizer, loss_fn, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in data_loader:\n",
        "            claim_input_ids = batch['claim_input_ids']\n",
        "            evidence_input_ids = batch['evidence_input_ids']\n",
        "            topic_input_ids = batch['topic_input_ids']\n",
        "            labels = batch['encoded_label']  # Use the new encoded_label\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(claim_input_ids, evidence_input_ids, topic_input_ids)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, data_loader, optimizer, loss_fn, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbmfnh1I1mp0"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_cepe(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the CEPE model on the given dataset.\n",
        "    Args:\n",
        "        model: Trained CEPE model.\n",
        "        data_loader: DataLoader for evaluation data.\n",
        "        device: Device to run the evaluation on (CPU or GPU).\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_claim_preds, all_claim_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Move inputs and labels to the appropriate device\n",
        "            claim_input_ids = batch['claim_input_ids'].to(device)\n",
        "            evidence_input_ids = batch['evidence_input_ids'].to(device)\n",
        "            topic_input_ids = batch['topic_input_ids'].to(device)\n",
        "            claim_labels = batch['claim_label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(claim_input_ids, evidence_input_ids, topic_input_ids)\n",
        "\n",
        "            # Predictions\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            # Append predictions and labels\n",
        "            all_claim_preds.extend(preds)\n",
        "            all_claim_labels.extend(claim_labels.cpu().numpy())\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Claim Classification Report:\")\n",
        "    print(classification_report(all_claim_labels, all_claim_preds, target_names=[\"Non-Match\", \"Match\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptwFZOLJ1rTu",
        "outputId": "f952940a-5fa0-49ec-bd67-ce6ed9625090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Non-Match       0.95      1.00      0.98      3016\n",
            "       Match       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.95      3165\n",
            "   macro avg       0.48      0.50      0.49      3165\n",
            "weighted avg       0.91      0.95      0.93      3165\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model_cepe(model, data_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
